# SentencePiece Go Port: Implementation Plan

## NOTE ON CODE

The C++ sources are available checked in to this repository at [sentencepiece/](../sentencepiece/).

The Gemini 3 data is available in [gemini-data/](../gemini-data/).

## NOTE ON PROGRESS

No stops are allowed in this work. Proceed without deviation, do NOT question the strategy.

> **Document status:** Complete plan to be finished in a strong effort.
> **References:** [`docs/12-sentencepiece-fuzzy-search-analysis.md`](../12-sentencepiece-fuzzy-search-analysis.md), [`docs/12.1-sentencepiece-quick-start.md`](../12.1-sentencepiece-quick-start.md), Rust baseline in [`docs/12.1.1-sentencepiece-rust.md`](./12.1.1-sentencepiece-rust.md).

## Executive Summary

### Goals
- Achieve functional parity with official SentencePiece C++ inference for tokenization
- Deliver <1 ms p95 tokenization for 280-character inputs with zero heap allocations on the hot path
- Provide pragmatic integration points for `go-server/` search workflows without introducing new service dependencies
- Maintain portable Go 1.22+ compatibility without CGO dependencies on the hot path

### Success Criteria
- Golden token outputs bit-match the C++ reference suite and are captured as reusable fixtures
- Benchmarks confirm sustained <1 ms p95 and zero allocations once the tokenizer MVP stabilises
- Public APIs support drop-in usage for tokenization with clear error contracts

## Objectives
- **Goal** Deliver a production-ready Go implementation of SentencePiece inference for semantic search in the MCP server.
- **Scope** Read-only inference path (no training), zero-allocation request handling, and integration points for `go-server/` services.
- **Success metrics** Match reference outputs, <1 ms p95 tokenization for 280 chars, zero heap allocations per request, and seamless drop-in usage for semantic fuzzy search pipelines.

> **Non-negotiable requirement:** The main inference hot path must execute with zero allocations. This is measured from Phase 3 onwards and formally gated during Phase 6 once the tokenizer MVP is in place.

## Execution Approach

- **Fixture-first validation.** Create and version golden token corpora so every step benchmarks against objective data.
- **Incremental zero-allocation tracking.** Record allocations from the first benchmarks and drive them to zero before broadening scope.
- **Dependency clarity.** Lock in core libraries (`google.golang.org/protobuf`, UTF-8 handling) early; treat stretch ideas (SIMD, ANN) as future work.

## Deliverables
- **Core library** `internal/sentencepiece` package exposing a zero-allocation `Processor` API.
- **Benchmark & test suite** Cross-language parity tests, microbenchmarks, fuzz/edge-case tests.
- **Integration guide** Documentation and sample wiring for `go-server/internal/search`.

## Testing Strategy
- **Unit coverage** Add focused tests under `go-server/pkg/sentencepiece/` for model parsing, trie search, normalisation transforms, and Viterbi backtracking (`*_test.go` with table-driven cases).
- **Golden fixtures** Reuse the C++ binaries in `sentencepiece/` and assets in `-sentence-piece-inference-tmp/` to generate `testdata/expected_tokens.json` and equivalent piece outputs for regression checks.
- **Parity harness** Provide a `TestEncodeParity` that loads the Go processor and validates outputs against the golden corpus, failing on any divergence.
- **Bench & fuzz guard** Extend `go test -bench Encode -benchmem` and Go fuzz targets to run in CI for allocation drift detection and robustness against malformed input.

## Public API Surface
- `Processor.Encode(ctx context.Context, input string) ([]int32, error)` and `Processor.EncodePieces(...)` for token IDs and surface pieces
- `LoadProcessor(modelPath string, opts ...Option) (*Processor, error)` with configurable normalization/token policies
- Integration helpers under `internal/search` for semantic reranking pipelines

## Error Handling & Observability
- Prefer sentinel errors (`ErrModelInvalid`, `ErrEncodeOverflow`) and wrap with `%w` when propagating
- All public APIs accept `context.Context` for cancellation; enforce `context.Err()` checks inside hot loops without allocation
- Keep observability lightweight: reuse existing `go-server` logging hooks outside the critical path and expose benchmark allocation counters via CI only

## Deferred Enhancements
- SIMD or hardware-specific optimisations once the baseline zero-allocation path is proven
- Embedding lookup helpers if downstream teams require bundled vectors
- Optional ANN backends kept under experimental build tags, subject to a separate staffing call

## Dependencies & Tooling
- **Proto codegen** Use `protoc` with `google.golang.org/protobuf/cmd/protoc-gen-go`; copy the vendored `sentencepiece_model.proto` from `/-sentence-piece-inference-tmp/` into the project source.
- **Build & lint** Go 1.22+, `golangci-lint` for static checks, `benchstat` for benchmark analysis.
- **Buffer reuse** `sync.Pool` for reusable slices in the tokenizer path.

## Suggested Package Structure (modifiable)

The following layout from earlier explorations is a starting point; teams may adapt directory names or module boundaries to match evolving needs while keeping zero-allocation expectations intact.

```
go-server/
├── pkg/
│   └── sentencepiece/
│       ├── sentencepiece.go        # Main API surface
│       ├── model.go                # ModelProto loading helpers
│       ├── normalizer.go           # Text normalization engine
│       ├── tokenizer.go            # Viterbi algorithm implementation
│       ├── trie.go                 # Double-array trie utilities
│       ├── embedding.go            # Embedding table accessors
│       ├── search.go               # Semantic search integration hook
│       ├── pool.go                 # Buffer pooling for zero-allocation calls
│       ├── proto/
│       │   ├── generate.go         # go:generate protoc entrypoint
│       │   ├── sentencepiece_model.pb.go
│       │   └── normalizer.pb.go
│       └── testdata/
│           ├── test.model
│           └── expected_outputs.json
└── internal/
    └── sentencepiece_bench/        # Internal benchmarks and profiling helpers
```

Document any deviations from this skeleton so long as pooled buffers remain co-located with the encode path to uphold zero-alloc guarantees.

## Implementation Phases

### Step 0: Research & Asset Preparation
- **Inventory models** Acquire baseline `.model` files referenced in `docs/12.1-sentencepiece-quick-start.md`; record source hashes in `testdata/README.md`.
- **Reference behaviour** Capture golden tokenisation outputs by running the official C++ binary over a curated corpus (tweets, multilingual, emoji, malformed inputs) and export expected IDs + pieces.
- **Exit criteria** Immutable fixtures (models, expected outputs) stored under `testdata/` with checksums and provenance notes.

### Step 1: Model Loading & Data Structures
- **Proto binding** Ensure Go structs from SentencePiece protos are generated; add helpers to decode normalization rules, special token IDs, and unigram/BPE parameters.
- **Vocabulary representation** Parse `ModelProto` into Go-friendly structs; store scores as `float32`, maintain both `[]byte` piece and UTF-8 rune spans.
- **Double-array trie** Port the Darts implementation with allocation-free traversal and validation tests against fixture lookups.
- **Configuration surface** Define `ProcessorConfig` options (dummy prefix, whitespace handling, byte fallback) derived from `NormalizerSpec` and `TrainerSpec`.
- **Exit criteria** `go test ./...` validates proto parsing and trie searches against fixtures with zero allocations under `-benchmem` smoke test.

### Step 2: Normalisation
- **Charsmap decoding** Recreate the normalisation trie from the proto’s serialized rules; implement longest-prefix match using the double-array trie over UTF-8 runes.
- **Buffer management** Pre-size normalisation buffers to `len(input)*3`; store position maps in `[]int` for backtracking while reusing pooled storage.
- **Whitespace logic** Implement `AddDummyPrefix`, `EscapeWhitespace`, `RemoveExtraWhitespace` options with tests covering ASCII, CJK, emoji, malformed UTF-8.
- **Fuzzing** Use Go fuzzing or `testing/quick` to ensure normalisation never panics on arbitrary byte sequences.
- **Exit criteria** Normalisation outputs match the C++ reference corpus; fuzz harness runs without panic for 1e5 inputs.

### Step 3: Tokenisation Engine
- **Optimised Viterbi** Implement the byte-oriented encoder mirroring `EncodeOptimized` with pooled buffers for state arrays, backpointers, and results.
- **UNK & byte fallback** Ensure byte-level tokens insert when trie misses; respect user-defined symbols configured in the model.
- **BPE compatibility** Implement greedy merge only if the target `.model` requires it; otherwise keep disabled.
- **Performance snapshot** Profile with `pprof` on representative texts; document current latency/allocations and iterate until zero allocations are confirmed.
- **Exit criteria** Golden token suite passes, `go test -bench Encode -benchmem` reports zero allocations after warm-up, and `TestEncodeParity` remains green.

### Step 4: Public API & Integration
- **Package surface** Expose `Processor.Encode(string) ([]int32, error)` and `Processor.EncodePieces(string) ([]string, error)`; keep API surface tight.
- **Initialisation** Implement `LoadProcessor(modelPath string, opts ...Option)` with lazy caches that steer clear of per-request allocations.
- **Server wiring** Integrate into `go-server/internal/search` pipeline to replace the existing tokenizer, using existing logging hooks for visibility outside the critical path.
- **Validation** Run parity suites, fuzzing, and benchmarks inside CI; enforce zero allocations via `-benchmem` guard before release.
- **Exit criteria** Integration demo passes end-to-end tests with zero allocations observed in benchmark CI.

## Research Notes & Open Questions
- **Trie reuse vs alternative** Evaluate existing Go double-array trie libraries; prefer in-house port if licence or performance concerns arise.
- **Concurrency strategy** Decide between per-request `Processor` instances vs shared singleton with pooled buffers; benchmark contention.
- **Deferred embeddings** If downstream teams need on-host embeddings, scope as a follow-up project with dedicated staffing.

## Integration Checklist
- **Model assets** Stored under `go-server/assets/sentencepiece/` with version hash.
- **Config knobs** Expose environment toggles (enable semantic search, token limit safeguards).
- **Operational runbooks** Document deployment steps, warm-up routines, and rollback plan if semantic search degrades results.
- **Future work** Explore streaming normalisation for very large bodies and optional embedding helpers once the tokenizer ships.
