# **Proposal: Stateful Aggregation for Semantically Charged Sub‑Millisecond Embeddings**

## **1. Motivation**

The current Model2Vec design achieves sub‑millisecond embeddings by averaging quantized token vectors. While this satisfies latency constraints, it underrepresents **compositional semantics** (e.g., *black cat* ≠ *black* + *cat*). To improve semantic fidelity without sacrificing deterministic latency, we propose a **stateful aggregation mechanism**: a lightweight residual state carried across tokens that modulates aggregation based on local affinities (e.g., adjective–noun, verb–object).

---

## **2. Architectural Overview**

### **2.1 Pipeline**
1. **Tokenization**: SentencePiece produces subword IDs.  
2. **Embedding Lookup**: Each ID maps to a quantized embedding vector plus a small metadata code (POS bucket, adjacency code).  
3. **Stateful Aggregation**:  
   - Maintain a fixed‑dimensional carry state.  
   - Update state per token.  
   - Apply affinity boosts and low‑rank pairwise interactions when semantically relevant tokens co‑occur.  
4. **Finalization**: Normalize or project the accumulated vector into the output embedding space.

### **2.2 Key Additions**
- **Carry State**: A small stack‑resident vector (e.g., 32D).  
- **Boost Matrix**: Prelearned multipliers for POS/role pairs.  
- **Window Kernel**: Decay function for token distance.  
- **Low‑Rank Pairwise Projector**: Captures collocations with minimal compute.  

All additions are cache‑resident and SIMD‑friendly.

---

## **3. Memory and Data Layout**

| Component | Size | Storage | Notes |
|-----------|------|---------|-------|
| Token Embeddings | 64B (PQ codes) | Static mmap | Cache‑resident |
| POS/Adjacency Codes | 1–2B per token | Inline with embedding | Zero‑copy access |
| Carry State | 32D f32 (128B) | Stack | Reset per request |
| Boost Matrix | K×K (e.g., 25 entries) | Static | Fits in L1 |
| Low‑Rank Maps (P,Q) | d×r + r×d | Static | r ≪ d (e.g., 16) |

---

## **4. Aggregation Algorithm**

### **4.1 Carry State Update**
\[
s \leftarrow \alpha s + \beta \cdot G(e_t, c_t)
\]
- \(e_t\): token embedding  
- \(c_t\): token code (POS/adjacency)  
- \(G\): lightweight projection (identity or masked copy)

### **4.2 Affinity Boost**
\[
\gamma_t = B[\text{pos}(t)][\text{pos}(h)] \cdot \omega(|t-h|) \cdot \sigma(u^\top s)
\]
- \(h\): most recent head token (noun/verb)  
- \(\omega\): window kernel  
- \(\sigma\): clamp to [1, b_max]

### **4.3 Pairwise Composition**
\[
h_{t,h} = Q^\top \big( (P e_t) \odot (P e_h) \big)
\]
- Low‑rank interaction, SIMD‑friendly  
- Added only for whitelisted POS pairs

### **4.4 Aggregation**
\[
a \leftarrow a + \gamma_t e_t + \sum_{h \in \mathcal{H}} h_{t,h}
\]

---

## **5. Rust Implementation Strategy**

- **Zero‑Allocation**:  
  - Token IDs: arena‑backed (`bumpalo::Vec<u32>`)  
  - Carry state & accumulator: stack‑backed (`ArrayVec<f32, D>`)  
- **SIMD Kernels**:  
  - PQ dequantization  
  - axpby for state update  
  - low‑rank projections (matvec)  
  - fused add for accumulation  
- **Alignment**: Ensure 32‑byte alignment for AVX2, 64‑byte for AVX‑512.

---

## **6. Training and Parameterization**

- **Objectives**:  
  - Contrastive sentence embedding loss  
  - Auxiliary composition loss (phrase salience)  
  - Order sensitivity regularization  
- **Learned Parameters**:  
  - Boost matrix \(B\)  
  - Window decay \(\lambda\)  
  - Low‑rank maps \(P, Q\)  
  - State gate vector \(u\)  
- **Deployment**: Quantize parameters to f16/int8; export as a single contiguous blob.

---

## **7. Performance Targets**

| Phase | Budget (µs) | Notes |
|-------|-------------|-------|
| Tokenization | 100 | SentencePiece FFI |
| Lookup | 150 | PQ codes, cache‑resident |
| Aggregation | 600 | SIMD, stateful boosts |
| Finalization | 50 | Normalize/project |
| **Total** | **<950** | p99.9 latency |

---

## **8. Evaluation Metrics**

- **Phrase Salience**: Embedding similarity for collocations vs. shuffled tokens.  
- **Retrieval Gains**: Recall@k on phrase‑heavy benchmarks.  
- **Latency Jitter**: p99.9 < 1ms under load.  
- **Ablations**: Compare baseline averaging vs. boost‑only vs. full stateful model.

---

## **9. Roadmap**

1. **Prototype Boost‑Only**: Implement POS‑based boosts, measure latency.  
2. **Add Carry State**: Introduce 32D state, calibrate α, β.  
3. **Low‑Rank Pairwise**: Add rank‑8 projector, profile SIMD kernel.  
4. **Training**: Fit parameters with contrastive + composition losses.  
5. **Hardening**: Alignment, cache profiling, jitter validation.  
6. **Packaging**: Export parameter blob, document binary layout.

---

## **Conclusion**

This proposal extends Model2Vec with a **stateful, semantically aware aggregation mechanism** that amplifies meaningful token interactions (e.g., *black cat*) while preserving the **sub‑millisecond, zero‑allocation, SIMD‑accelerated** design. It offers a pragmatic middle ground between bag‑of‑subwords averaging and full transformer attention, delivering richer sentence embeddings at HPC‑grade latency.

