# Implementation Plan: Sub-Millisecond Model2Vec Embeddings

This plan outlines the steps to implement the zero-allocation, quantized embedding engine as specified in the architectural proposal. The goal is to achieve sub-millisecond inference latency by leveraging a `Model2Vec` approach with SentencePiece tokenization.

---

## Current Status

**What you have:**
- `gemma-data/tokenizer.model` + `tokenizer.json` — SentencePiece tokenizer (working)
- `rust-server/src/sentencepiece/*` — Native Rust tokenizer implementation

**What's missing:**
- **Gemma model weights** (`model.safetensors` or `.bin` files) — you only have the tokenizer, not the embedding weights
- Extracted embedding matrix
- PCA/PQ artifacts
- Zero-alloc Rust inference engine

---

### **Phase 0: Acquire Model Weights**

You need the actual Gemma model weights to extract the token embedding matrix.

**Options:**

1. **Download from Hugging Face** (recommended):
   ```bash
   # Install huggingface-cli
   pip install huggingface-hub
   
   # Download Gemma model (requires acceptance of license + HF token)
   huggingface-cli login
   huggingface-cli download google/gemma-2b --local-dir ./gemma-model
   ```

2. **Alternative: Use a pre-extracted embedding file**:
   - Search HF for repos with pre-extracted embeddings (.npy files)
   - Or use fastText/GloVe/word2vec embeddings (simpler but less aligned with Gemma)

3. **Alternative: Use a smaller compatible model**:
   - Download a smaller open model with safetensors (e.g., GPT-2, OPT, Pythia)
   - Extract embeddings from there

**Once you have model.safetensors:**
- Run the extractor tool: `cd tools/extract_embeddings && cargo run --release`
- This produces `gemma-data/embeddings.npy`

---

### **Phase 0b: Offline Dimensionality Reduction (Optional but Recommended)**

Create a simple Python script to reduce dimensions via PCA (one-time):

```python
import numpy as np
from sklearn.decomposition import PCA

# Load raw embeddings
emb = np.load("gemma-data/embeddings.npy")
print("Original shape:", emb.shape)

# Fit PCA
pca = PCA(n_components=128)
emb_reduced = pca.fit_transform(emb)

# Save
np.save("gemma-data/embeddings_pca128.npy", emb_reduced)
print("Reduced shape:", emb_reduced.shape)
print("Explained variance:", pca.explained_variance_ratio_.sum())
```

This gives you a 128-dim embedding table that's much faster to work with.

---

### **Phase 1: Rust - Baseline Prototype (Float Embeddings)**

Goal: prove the pipeline works with simple float embeddings before adding PQ/SIMD complexity.

1. **Add memory-mapping loader**:
   - Create `rust-server/src/embeddings/loader.rs`
   - Use `memmap2` to mmap `embeddings_pca128.npy`
   - Parse .npy header, expose embeddings as `&[f32]` slice

2. **Implement baseline `embed()` function**:
   ```rust
   pub fn embed(text: &str, tokenizer: &SentencePieceProcessor, embeddings: &EmbeddingTable) -> Vec<f32> {
       let tokens = tokenizer.encode(text);
       let mut acc = vec![0.0f32; 128];
       for token_id in tokens {
           let emb = embeddings.get(token_id);
           for i in 0..128 {
               acc[i] += emb[i];
           }
       }
       // Average and normalize
       let count = tokens.len() as f32;
       for x in &mut acc {
           *x /= count;
       }
       normalize(&mut acc);
       acc
   }
   ```

3. **Test**:
   - Create a simple CLI or unit test
   - Measure baseline latency (likely 1-5ms without optimization)

---

### **Phase 2: Zero-Allocation Integration**

1. **Arena-allocate token IDs**:
   - Modify `SentencePieceProcessor::encode` to accept `&Bump` arena
   - Return `bumpalo::collections::Vec<'a, u32>` instead of `Vec<u32>`

2. **Stack-allocate output**:
   - Change return type to `ArrayVec<f32, 128>`
   - Allocate accumulator on arena or stack

3. **Benchmark**:
   - Use `criterion.rs` to measure p50/p99/p99.9 latency
   - Target: <2ms p99 at this stage

---

### **Phase 3: Product Quantization (Advanced)**

**Only proceed here if you need sub-1ms latency and baseline is too slow.**

This requires:
- Training PQ codebooks offline (Python + FAISS recommended)
- Storing quantized codes (u8 per subvector)
- Implementing dequantization + aggregation in Rust

See original Phase 0 for Python PQ training pipeline details.

---

### **Phase 4: SIMD Optimization (Advanced)**

**Only needed if profiling shows aggregation is the bottleneck.**

- Use `std::arch` intrinsics (AVX2/AVX-512)
- Manually vectorize the accumulation loop
- Ensure alignment of buffers

---

### **Phase 5: Stateful Carry (Optional Enhancement)**

Implement the carry-state logic from `12.2.6-carry-state.md`:
- Add POS tagging heuristic
- Implement boost matrix B
- Add small carry state s with update rule
- Measure semantic improvement vs latency cost

---

## Next Immediate Steps

1. **Download Gemma model weights** or choose an alternative embedding source
2. **Run the extractor tool** to get `embeddings.npy`
3. **Optionally run PCA** to get 128-dim version
4. **Implement baseline Rust loader + embed()** (Phase 1)
5. **Benchmark and validate** before adding complexity

---

## Decision Points

- **Skip PQ/SIMD if baseline is fast enough** — measure first, optimize later
- **Skip carry-state if simple averaging gives good enough semantic results** — A/B test
- **Consider using pre-trained sentence-transformers models** if downloading full Gemma is too heavy

1.  **Setup Python Environment**:
    *   Install necessary libraries: `torch`, `safetensors`, `sentencepiece`, `numpy`, `scikit-learn`.

2.  **Create Extraction Script**:
    *   Load the SentencePiece tokenizer from `gemma-data/tokenizer.model`.
    *   Load the Gemma model weights from the `.safetensors` file.
    *   Extract the raw token embedding tensor, typically named `model.embed_tokens.weight`. Its dimensions will be `[vocab_size, embedding_dim]`.

3.  **Dimensionality Reduction (PCA)**:
    *   Apply Principal Component Analysis (PCA) to the extracted tensor to reduce its dimensionality (e.g., from 3072 to 128).
    *   Retain and save the PCA transformation matrix.

4.  **Quantization (Product Quantization)**:
    *   Apply Product Quantization (PQ) to the dimension-reduced embedding table.
    *   Partition each vector into sub-vectors and generate codebooks for each partition.
    *   The final table will consist of `u8` code indices instead of `f32` values.
    *   Save the PQ codebooks.

5.  **Serialize to Custom Binary Format**:
    *   Define a binary file format to store all necessary assets for the Rust engine.
    *   The file should contain:
        *   A header with metadata (e.g., final embedding dimension, number of PQ partitions).
        *   The PCA transformation matrix.
        *   The PQ codebooks.
        *   The main quantized embedding table (`[vocab_size, num_partitions]`).
    *   Write all components into a single `.bin` file. This will be the only artifact the Rust engine needs.

---

### **Phase 1: Rust - Core Data Structures & Loading**

This phase prepares the Rust application to efficiently load and access the generated assets.

1.  **Add Dependencies**:
    *   Add `memmap2`, `bumpalo`, and `arrayvec` to `Cargo.toml`.

2.  **Define Data Structures**:
    *   Create Rust `struct`s with `#[repr(C)]` that exactly mirror the layout of the custom binary file created in Phase 0. This allows for safe, zero-copy casting from the memory-mapped file.

3.  **Implement Asset Loader**:
    *   Create a loader module (e.g., `embedding_engine::loader`).
    *   Implement a function that takes the path to the `.bin` file, memory-maps it using `memmap2::Mmap`, and returns a struct providing safe slice access (`&[T]`) to the PCA matrix, PQ codebooks, and the main embedding table.

---

### **Phase 2: Rust - Zero-Allocation Inference Pipeline**

This phase builds the core logic for processing a request without heap allocations.

1.  **Create the `EmbeddingEngine`**:
    *   Define a main `EmbeddingEngine` struct that holds the loaded assets from Phase 1.
    *   Instantiate this engine once at application startup.

2.  **Adapt SentencePiece Processor**:
    *   Modify the existing `SentencePieceProcessor::encode` method to accept a reference to a `&'a bumpalo::Bump` arena.
    *   Change its return type from `Vec<u32>` to `bumpalo::collections::Vec<'a, u32>` to ensure the list of token IDs is allocated on the arena.

3.  **Implement the `embed` Function**:
    *   Create the main entry point for inference: `pub fn embed<'a>(&self, text: &str, arena: &'a Bump) -> ArrayVec<f32, D_OUT>`.
    *   **Step 1 (Tokenize):** Call the modified SentencePiece processor to get the token IDs in an arena-allocated vector.
    *   **Step 2 (Lookup):** Iterate through the token IDs. For each ID, perform a direct index lookup into the memory-mapped quantized embedding table to get its PQ code indices.

---

### **Phase 3: Rust - High-Performance SIMD Aggregation**

This phase implements the computationally intensive vector aggregation using CPU-level parallelism.

1.  **Implement a Baseline Aggregation Loop**:
    *   First, write a simple, safe Rust loop to verify the logic. For each token:
        1.  Retrieve its PQ codes.
        2.  Use the codes to look up the corresponding sub-vectors from the PQ codebooks.
        3.  Reconstruct the full-precision (but dimension-reduced) vector.
        4.  Sum the reconstructed vector into an intermediate `f32` buffer.
    *   This buffer should be allocated on the arena: `bumpalo::vec![0.0f32; D_OUT]`.

2.  **Develop the SIMD Kernel**:
    *   Refactor the aggregation loop into an `unsafe` function that uses `std::arch` intrinsics (e.g., for AVX2 on `x86_64`).
    *   The loop should process vectors in chunks matching the SIMD register width (e.g., 8 `f32`s for a 256-bit register).
    *   Use intrinsics like `_mm256_load_ps`, `_mm256_add_ps`, and `_mm256_store_ps` for maximum throughput.
    *   **Crucially:** Ensure the intermediate summation buffer is correctly aligned to a 32-byte boundary to prevent performance penalties from unaligned memory access. `bumpalo` provides methods for aligned allocation.

3.  **Finalize the Vector**:
    *   After the loop, perform a final averaging of the summed vector.
    *   Apply L2 normalization to produce the final unit vector.
    *   Copy the result into a stack-allocated `ArrayVec` for the return value.

---

### **Phase 4: Benchmarking and Validation**

This phase validates that the implementation meets the strict sub-millisecond performance target.

1.  **Create Benchmarks with `criterion.rs`**:
    *   Write a benchmark that measures the end-to-end latency of the `embed` function for a typical input (~300 characters).
    *   The benchmark setup should initialize the `EmbeddingEngine` and a `Bump` arena.

2.  **Measure Latency and Jitter**:
    *   Focus not just on the average latency but on the entire distribution.
    *   The key success metric is ensuring the 99.9th percentile (p99.9) latency remains below 1000µs, proving the elimination of jitter.

3.  **Profile and Optimize**:
    *   Use profiling tools (e.g., `perf`, `flamegraph`) to identify any remaining hot spots in the SIMD kernel or lookup phase.
    *   Ensure the compiler is generating the expected vectorized instructions (`-C target-cpu=native`).