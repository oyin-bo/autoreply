# SentencePiece Fuzzy Search: Quick Start Guide

## Overview

This guide provides a quick reference for implementing SentencePiece-based semantic fuzzy search. For the full analysis, see [12-sentencepiece-fuzzy-search-analysis.md](./12-sentencepiece-fuzzy-search-analysis.md).

## TL;DR

**Goal:** Enable semantic search where "foot" matches "feet", "run" matches "running", etc.

**Solution:** Two-stage approach:
1. **SentencePiece tokenization** - Convert text to token IDs
2. **Static embeddings** - Convert token IDs to semantic vectors

**Implementation:** Port SentencePiece inference to Go/Rust/JS + use pre-computed embedding table.

---

## Quick Architecture

```
┌─────────────┐
│  Query Text │
└──────┬──────┘
       │
       ▼
┌─────────────────────┐
│  1. Normalization   │  Unicode cleanup, whitespace handling
└──────┬──────────────┘
       │
       ▼
┌─────────────────────┐
│  2. Tokenization    │  Viterbi algorithm → Token IDs
└──────┬──────────────┘
       │
       ▼
┌─────────────────────┐
│  3. Embedding       │  Token IDs → 64D vector
│     Lookup          │  (average + normalize)
└──────┬──────────────┘
       │
       ▼
┌─────────────────────┐
│  4. Similarity      │  Dot product with post embeddings
│     Calculation     │  → Ranked results
└─────────────────────┘
```

---

## Core Components to Implement

### 1. Normalizer
- Input: Raw text
- Output: Normalized text + position map
- Key: Trie-based character mapping
- Complexity: ~200 lines of code

### 2. Tokenizer (Unigram/Viterbi)
- Input: Normalized text
- Output: Token IDs
- Key: Viterbi algorithm on lattice
- Complexity: ~400 lines of code

### 3. Trie (Double-Array)
- Input: Vocabulary list
- Output: Fast prefix search structure
- Key: Compact array representation
- Complexity: ~300 lines of code

### 4. Embedding Table
- Input: Pre-computed embeddings file
- Output: Memory-mapped array
- Key: O(1) lookup by token ID
- Complexity: ~100 lines of code

**Total Core Code:** ~1000 lines per language

---

## Implementation Order

### Phase 1: Rust Reference (2 weeks)
```rust
// Core API
pub struct SentencePieceProcessor {
    model: ModelProto,
    vocab_trie: DoubleArrayTrie,
    norm_trie: DoubleArrayTrie,
}

impl SentencePieceProcessor {
    pub fn encode(&self, text: &str) -> Vec<u32> { ... }
}

pub struct EmbeddingTable {
    embeddings: Mmap,  // Memory-mapped file
    dim: usize,
}

impl EmbeddingTable {
    pub fn get_embedding(&self, tokens: &[u32]) -> Vec<f32> { ... }
}
```

### Phase 2: Go Port (1 week)
```go
type SentencePieceProcessor struct {
    model      *ModelProto
    vocabTrie  *DoubleArrayTrie
    normTrie   *DoubleArrayTrie
}

func (sp *SentencePieceProcessor) Encode(text string) []uint32 { ... }

type EmbeddingTable struct {
    embeddings []float32  // Memory-mapped
    dim        int
}

func (et *EmbeddingTable) GetEmbedding(tokens []uint32) []float32 { ... }
```

### Phase 3: JS/TypeScript (1 week)
```typescript
class SentencePieceProcessor {
    encode(text: string): Uint32Array { ... }
}

class EmbeddingTable {
    getEmbedding(tokens: Uint32Array): Float32Array { ... }
}
```

---

## Data Files Needed

### 1. SentencePiece Model (~5MB)
```
sp-32k-multilingual.model
├── Vocabulary (32k tokens)
├── Token scores (log probabilities)
├── Normalization rules
└── Configuration
```

**Source:** Use existing multilingual model or train on social media corpus

### 2. Embedding Table (~2MB)
```
sp-32k-embeddings-64d.bin
├── Header (64 bytes)
│   ├── Magic: "SPEM"
│   ├── Version: 1
│   ├── Vocab size: 32000
│   └── Embedding dim: 64
└── Embeddings (32000 × 64 × 4 bytes)
```

**Generation:**
```python
# Pseudo-code
teacher_model = load_teacher("multilingual-e5-base")
embeddings = []
for token in vocabulary:
    embedding = teacher_model.encode(token)
    embeddings.append(embedding)

# PCA reduction 768D → 64D
embeddings_64d = pca_reduce(embeddings, target_dim=64)

# Save to binary
save_embeddings(embeddings_64d, "sp-32k-embeddings-64d.bin")
```

---

## Testing Checklist

### Correctness
- [ ] Matches official SentencePiece output on test suite
- [ ] Handles Unicode (Latin, CJK, emoji) correctly
- [ ] Doesn't crash on malformed input

### Performance
- [ ] <1ms tokenization for 280 chars (p95)
- [ ] <0.1ms embedding lookup for 50 tokens
- [ ] <100ms full search for 100 posts

### Quality
- [ ] "foot" matches "feet" with >0.7 similarity
- [ ] "run" matches "running" with >0.8 similarity
- [ ] Multilingual: English, emoji, CJK work correctly

---

## Example Usage

### Tokenization
```rust
let sp = SentencePieceProcessor::load("model.model")?;
let tokens = sp.encode("Hello world");
// Output: [72, 8661, 99, 934]
```

### Semantic Search
```rust
let sp = SentencePieceProcessor::load("model.model")?;
let embeddings = EmbeddingTable::load("embeddings.bin")?;

// Index posts
let mut post_index = Vec::new();
for post in posts {
    let tokens = sp.encode(&post.text);
    let embedding = embeddings.get_embedding(&tokens);
    post_index.push((post.id, embedding));
}

// Search
fn search(query: &str, top_k: usize) -> Vec<PostId> {
    let query_tokens = sp.encode(query);
    let query_embedding = embeddings.get_embedding(&query_tokens);
    
    let mut scores: Vec<_> = post_index.iter()
        .map(|(id, emb)| (*id, dot_product(&query_embedding, emb)))
        .collect();
    
    scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
    scores.truncate(top_k);
    
    scores.into_iter().map(|(id, _)| id).collect()
}
```

---

## Key Algorithms

### Viterbi Tokenization (Simplified)
```pseudocode
function tokenize(text):
    nodes = array of size (text.length + 1)
    nodes[0] = {score: 0, token_id: -1, starts_at: -1}
    
    for pos in 0..text.length:
        matches = vocab_trie.find_all_starting_at(text[pos:])
        
        for match in matches:
            end_pos = pos + match.length
            new_score = nodes[pos].score + vocab_scores[match.id]
            
            if new_score > nodes[end_pos].score:
                nodes[end_pos] = {
                    score: new_score,
                    token_id: match.id,
                    starts_at: pos
                }
    
    // Backtrack
    tokens = []
    pos = text.length
    while pos > 0:
        tokens.prepend(nodes[pos].token_id)
        pos = nodes[pos].starts_at
    
    return tokens
```

### Embedding Lookup
```pseudocode
function get_embedding(token_ids):
    sum = zeros(embedding_dim)
    
    for token_id in token_ids:
        embedding = embedding_table[token_id]
        sum += embedding
    
    // Average pool
    sum /= len(token_ids)
    
    // L2 normalize
    norm = sqrt(sum.dot(sum))
    sum /= norm
    
    return sum
```

### Similarity Calculation
```pseudocode
function cosine_similarity(vec1, vec2):
    // Since both are L2-normalized, dot product = cosine similarity
    return dot_product(vec1, vec2)
```

---

## Performance Tips

### Memory Optimization
- Use buffer pools for per-request allocations
- Memory-map embedding table (no load into RAM)
- Pre-allocate with capacity for typical inputs

### Speed Optimization
- SIMD for dot products (Rust: `std::simd`)
- Parallel search across posts (Rust: `rayon`)
- Cache post embeddings (compute once)

### Size Optimization
- Quantize embeddings to int8 (2MB → 512KB)
- Use smaller vocabulary if acceptable
- PCA reduction (768D → 64D gives 12× compression)

---

## Common Pitfalls

1. **UTF-8 vs Character Positions**
   - SentencePiece works in Unicode characters, not bytes
   - Must convert carefully between byte and char positions

2. **Trie Memory Layout**
   - Double-Array Trie is complex, consider using library
   - Or implement simpler prefix tree for first version

3. **Embedding Quality**
   - Teacher model matters! Use multilingual-e5-base or better
   - Domain-specific fine-tuning may help

4. **Normalization Edge Cases**
   - Emoji sequences (ZWJ, skin tones)
   - Invalid UTF-8
   - Mixed RTL/LTR text

---

## Resources

### Documentation
- Full analysis: [12-sentencepiece-fuzzy-search-analysis.md](./12-sentencepiece-fuzzy-search-analysis.md)
- Model2Vec approach: [3-detour-model2vec.md](./3-detour-model2vec.md)
- Tokenization deep dive: [5-detour-tokenisation-stencepiece.md](./5-detour-tokenisation-stencepiece.md)

### Code Reference
- Original C++ implementation: `/-sentence-piece-inference-tmp/`
- Key files to study:
  - `unigram_model.cc` - Viterbi algorithm
  - `normalizer.cc` - Text normalization
  - `model_interface.cc` - Vocabulary management

### Papers
- SentencePiece: [arxiv.org/abs/1808.06226](https://arxiv.org/abs/1808.06226)
- BPE: [arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909)
- Model2Vec: Static embeddings approach

---

## Next Steps

1. **Immediate (Week 1)**
   - [ ] Set up Rust development environment
   - [ ] Implement Protobuf parser for ModelProto
   - [ ] Implement basic trie (or find library)

2. **Short-term (Weeks 2-3)**
   - [ ] Implement normalizer
   - [ ] Implement Viterbi tokenizer
   - [ ] Test against official SentencePiece

3. **Medium-term (Weeks 4-6)**
   - [ ] Generate embedding table
   - [ ] Implement embedding lookup
   - [ ] Build search API

4. **Long-term (Weeks 7-8)**
   - [ ] Port to Go
   - [ ] Port to JS/WASM
   - [ ] Integration with MCP server

---

## Questions?

If you need clarification on any part of the implementation:
1. Refer to the full analysis document
2. Study the original C++ code in `/-sentence-piece-inference-tmp/`
3. Test against official SentencePiece for validation

**Remember:** Start simple, test thoroughly, optimize later!
