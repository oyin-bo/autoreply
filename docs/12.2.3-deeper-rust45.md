# Rust Implementation Research: Semantic Annealing Engine

**Research Date:** 2025-10-03  
**Target:** Rust tools, patterns, and libraries for implementing the Semantic Annealing Engine

---

## Executive Summary

The Semantic Annealing Engine requires:
1. **Zero-allocation runtime** with GPU compatibility
2. **SentencePiece tokenization**
3. **Pattern matching and rewriting** on linear buffers
4. **Stack-based VM** for embedding generation
5. **Evolutionary algorithm** for rule discovery

Rust is exceptionally well-suited for this task due to its zero-cost abstractions, explicit memory control, and GPU interop capabilities.

---

## 1. Core Architecture Patterns

### 1.1 Zero-Allocation Runtime Design

**Pattern: Arena Allocation with Fixed Buffers**

```rust
// Use fixed-size arrays or pre-allocated buffers
struct ProgramStream {
    items: [ProgramItem; MAX_ITEMS],
    len: usize,
}

// Or use bumpalo for arena allocation in non-critical paths
use bumpalo::Bump;
```

**Key Crates:**
- **`bumpalo`** (v3.14+): Arena allocator for setup phases
- **`arrayvec`** (v0.7+): Stack-allocated vectors with compile-time capacity
- **`smallvec`** (v1.13+): Small vector optimization (stack storage for small sizes)
- **`tinyvec`** (v1.6+): 100% safe alternative to arrayvec

**Recommendation:** Use fixed-size arrays (`[T; N]`) for the main `ProgramStream` buffer to guarantee zero allocation. Use `arrayvec` for temporary collections during rule matching.

### 1.2 GPU Compatibility

**Pattern: CUDA/ROCm Integration**

**Key Crates:**
- **`cudarc`** (v0.11+): Safe CUDA bindings for Rust
- **`wgpu`** (v0.19+): Cross-platform GPU API (WebGPU standard)
- **`vulkano`** (v0.34+): Safe Vulkan wrapper
- **`rust-cuda`** (v0.3+): Full CUDA support with kernel compilation

**For Semantic Annealing:**
```rust
// Option 1: CUDA for NVIDIA GPUs
use cudarc::driver::*;
use cudarc::nvrtc::*;

// Option 2: wgpu for cross-platform
use wgpu::{Device, Queue, Buffer};
```

**Recommendation:** Start with **`cudarc`** for NVIDIA-specific optimization, with **`wgpu`** as fallback for broader compatibility. The annealing loop's pattern-matching is highly parallelizable across multiple input texts.

### 1.3 Memory Layout for Cache Efficiency

**Pattern: Structure of Arrays (SoA)**

```rust
// Instead of Array of Structures (AoS):
struct ProgramItem {
    instruction: u8,
    token_id: u32,
    score: f32,
}

// Use Structure of Arrays for better cache locality:
struct ProgramStream {
    instructions: [u8; MAX_ITEMS],
    token_ids: [u32; MAX_ITEMS],
    scores: [f32; MAX_ITEMS],
    len: usize,
}
```

**Key Crates:**
- **`soa_derive`** (v0.13+): Automatic SoA generation from struct definitions

---

## 2. SentencePiece Tokenization

### 2.1 Native Rust Implementation

**Key Crates:**
- **`sentencepiece`** (v0.11+): Pure Rust implementation
- **`tokenizers`** (v0.15+): HuggingFace's fast tokenizer library (includes SentencePiece)

**Comparison:**

| Crate | Pros | Cons |
|-------|------|------|
| `sentencepiece` | Pure Rust, no C++ deps | Less mature, fewer features |
| `tokenizers` | Battle-tested, very fast | Larger dependency tree |

**Recommendation:** Use **`tokenizers`** from HuggingFace for production quality:

```rust
use tokenizers::Tokenizer;

let tokenizer = Tokenizer::from_file("model.json")?;
let encoding = tokenizer.encode("input text", false)?;
let token_ids: &[u32] = encoding.get_ids();
```

### 2.2 C++ Binding Alternative

If using the original Google SentencePiece:

**Key Crates:**
- **`sentencepiece-sys`** (v0.1+): FFI bindings to C++ library
- **`cc`** (v1.0+): Build script integration for C++ compilation

**Note:** The workspace already has SentencePiece C++ files in `-sentence-piece-inference-tmp/`. Consider:

```rust
// Build script integration
// build.rs
fn main() {
    cc::Build::new()
        .cpp(true)
        .file("../sentence-piece-inference-tmp/bpe_model.cc")
        .compile("sentencepiece");
}
```

---

## 3. Pattern Matching and Rewrite Rules

### 3.1 Efficient Pattern Matching

**Pattern: Trie-based Multi-Pattern Matching**

**Key Crates:**
- **`aho-corasick`** (v1.1+): Multi-pattern string matching (already in Cargo.toml)
- **`fst`** (v0.4+): Finite state transducers for pattern matching
- **`radix_trie`** (v0.2+): Radix trie for prefix matching
- **`qp-trie`** (v0.8+): Fast trie implementation

**For Token Sequences:**
```rust
use aho_corasick::AhoCorasick;

// Build pattern matcher for rewrite rules
let patterns = vec![
    vec![TOKEN_THE, TOKEN_CAT],
    vec![TOKEN_A, TOKEN_DOG],
];
let ac = AhoCorasick::new(patterns)?;

// Find all matches in program stream
for mat in ac.find_iter(&token_stream) {
    // Apply rewrite rule
}
```

**Recommendation:** Use **`aho-corasick`** for multi-pattern matching of token sequences. It's already in your dependencies and extremely fast.

### 3.2 Rule Representation

**Pattern: Compact Rule Encoding**

```rust
#[repr(C)]
struct RewriteRule {
    pattern_start: u32,      // Index into pattern buffer
    pattern_len: u16,
    action: RewriteAction,
    score: f32,
}

#[repr(u8)]
enum RewriteAction {
    Merge { new_instruction: u8 },
    Reorder { swap_distance: u8 },
    Replace { new_instruction: u8, new_token: u32 },
}

// All rules stored in contiguous array
struct RuleSet {
    rules: Vec<RewriteRule>,
    patterns: Vec<u32>,  // Flat buffer of all patterns
}
```

### 3.3 Bid Selection and Scoring

**Pattern: Priority Queue for Best Bid**

**Key Crates:**
- **`priority-queue`** (v2.0+): Priority queue with changeable priorities
- **`binary-heap-plus`** (v0.5+): Extended binary heap

```rust
use priority_queue::PriorityQueue;

struct Bid {
    rule_id: u32,
    position: usize,
    score: f32,
}

let mut bids = PriorityQueue::new();
// Add bids with scores as priorities
bids.push(bid, OrderedFloat(bid.score));

// Get best bid
if let Some((best_bid, _)) = bids.pop() {
    // Apply rewrite
}
```

---

## 4. Stack-Based Virtual Machine

### 4.1 Forth VM Implementation

**Pattern: Direct Threaded Interpreter**

```rust
type StackValue = f32;

struct ForthVM {
    stack: [StackValue; 256],
    sp: usize,
    embedding: [f32; EMBEDDING_DIM],
}

impl ForthVM {
    fn execute(&mut self, program: &ProgramStream) {
        for item in &program.items[..program.len] {
            match item.instruction {
                OP_PUSH => self.push(item.value),
                OP_ADD => {
                    let b = self.pop();
                    let a = self.pop();
                    self.push(a + b);
                }
                OP_EMBED => {
                    let idx = self.pop() as usize;
                    self.embedding[idx] = self.pop();
                }
                _ => {}
            }
        }
    }
}
```

**Key Crates:**
- **`num-traits`** (v0.2+): Generic numeric operations
- **`half`** (v2.3+): f16 support for compact embeddings

### 4.2 SIMD Optimization

**Key Crates:**
- **`std::simd`** (nightly): Portable SIMD
- **`wide`** (v0.7+): SIMD types on stable Rust
- **`simdeez`** (v2.0+): Runtime SIMD detection

```rust
use wide::f32x8;

// Vectorized embedding operations
fn add_to_embedding(embedding: &mut [f32], values: &[f32]) {
    for (chunk_e, chunk_v) in embedding
        .chunks_exact_mut(8)
        .zip(values.chunks_exact(8))
    {
        let e = f32x8::from(chunk_e);
        let v = f32x8::from(chunk_v);
        (e + v).write_to_slice(chunk_e);
    }
}
```

**Recommendation:** Use **`wide`** for stable Rust SIMD support in the VM's embedding operations.

---

## 5. Backtracking and State Management

### 5.1 Efficient State Snapshots

**Pattern: Copy-on-Write with Small State**

```rust
// Option 1: Direct copy (if state is small)
struct ProgramState {
    stream: ProgramStream,  // If < 1KB, just copy
    iteration: u32,
}

impl ProgramState {
    fn snapshot(&self) -> Self {
        *self  // Copy entire state
    }
}

// Option 2: Incremental snapshots
struct StateHistory {
    base: ProgramStream,
    deltas: Vec<Delta>,
}

enum Delta {
    ItemChanged { index: usize, old: ProgramItem, new: ProgramItem },
    ItemsSwapped { i: usize, j: usize },
}
```

**Key Crates:**
- **`im`** (v15.1+): Immutable data structures with structural sharing
- **`rpds`** (v1.1+): Persistent data structures

**Recommendation:** For the annealing loop, direct copying is likely fastest if `ProgramStream` is kept small (< 4KB). Use a ring buffer for history:

```rust
struct AnnealingState {
    current: ProgramStream,
    history: [ProgramStream; 8],  // Last 8 states
    history_idx: usize,
}
```

### 5.2 Rule Inhibition

**Pattern: Bloom Filter for Fast Lookup**

**Key Crates:**
- **`bloomfilter`** (v1.0+): Space-efficient bloom filter
- **`probabilistic-collections`** (v0.7+): Various probabilistic data structures

```rust
use bloomfilter::Bloom;

struct InhibitedRules {
    filter: Bloom<u32>,
    exact: Vec<u32>,  // Fallback for critical accuracy
}

impl InhibitedRules {
    fn inhibit(&mut self, rule_id: u32) {
        self.filter.set(&rule_id);
        self.exact.push(rule_id);
    }
    
    fn is_inhibited(&self, rule_id: u32) -> bool {
        self.filter.check(&rule_id) && self.exact.contains(&rule_id)
    }
}
```

---

## 6. Evolutionary Algorithm Engine

### 6.1 Genetic Algorithm Framework

**Key Crates:**
- **`genevo`** (v0.7+): Genetic algorithm framework
- **`rsgenetic`** (v2.0+): Simple genetic algorithm library
- **`genetic-algorithm`** (v0.1+): Minimalist GA implementation

**Recommendation:** Use **`genevo`** for its flexibility:

```rust
use genevo::prelude::*;

#[derive(Clone, Debug)]
struct Algorithm {
    rules: Vec<RewriteRule>,
    weights: Vec<f32>,
}

impl Genotype for Algorithm {
    type Dna = Vec<u8>;
    // Implement genetic operations
}

struct AlgorithmFitness;

impl FitnessFunction<Algorithm, f32> for AlgorithmFitness {
    fn fitness_of(&self, algorithm: &Algorithm) -> f32 {
        // Run runtime engine on corpus
        // Compare to teacher embeddings
        calculate_similarity(algorithm)
    }
}
```

### 6.2 Parallel Fitness Evaluation

**Key Crates:**
- **`rayon`** (v1.8+): Data parallelism
- **`crossbeam`** (v0.8+): Concurrent data structures

```rust
use rayon::prelude::*;

fn evaluate_population(population: &[Algorithm]) -> Vec<f32> {
    population
        .par_iter()
        .map(|algo| evaluate_fitness(algo))
        .collect()
}
```

**Recommendation:** Use **`rayon`** for embarrassingly parallel fitness evaluation across the population.

### 6.3 Corpus Management

**Key Crates:**
- **`memmap2`** (v0.9+): Memory-mapped file I/O
- **`bincode`** (v1.3+): Fast binary serialization
- **`rkyv`** (v0.7+): Zero-copy deserialization

```rust
use memmap2::Mmap;
use rkyv::{Archive, Deserialize, Serialize};

#[derive(Archive, Deserialize, Serialize)]
struct CorpusEntry {
    text: String,
    teacher_embedding: [f32; 768],
}

// Memory-map large corpus file
let file = File::open("corpus.bin")?;
let mmap = unsafe { Mmap::map(&file)? };
let corpus: &[CorpusEntry] = rkyv::from_bytes(&mmap)?;
```

**Recommendation:** Use **`rkyv`** for zero-copy access to large corpus files during fitness evaluation.

---

## 7. Embedding Similarity Metrics

### 7.1 Vector Operations

**Key Crates:**
- **`ndarray`** (v0.15+): N-dimensional arrays
- **`nalgebra`** (v0.32+): Linear algebra
- **`faer`** (v0.18+): Fast linear algebra (newer, very fast)

```rust
use ndarray::Array1;

fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    let dot: f32 = a.iter().zip(b).map(|(x, y)| x * y).sum();
    let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();
    dot / (norm_a * norm_b)
}
```

**Recommendation:** Implement cosine similarity directly for simplicity. Use **`faer`** if you need matrix operations for batch processing.

### 7.2 Approximate Nearest Neighbor (for validation)

**Key Crates:**
- **`hnsw`** (v0.11+): Hierarchical Navigable Small World graphs
- **`instant-distance`** (v0.6+): Fast ANN search

---

## 8. Serialization and Persistence

### 8.1 Algorithm Storage

**Key Crates:**
- **`serde`** (v1.0+): Already in Cargo.toml
- **`bincode`** (v1.3+): Fast binary format
- **`postcard`** (v1.0+): No-std friendly binary format
- **`rmp-serde`** (v1.1+): MessagePack format

```rust
use serde::{Deserialize, Serialize};
use bincode;

#[derive(Serialize, Deserialize)]
struct SerializedAlgorithm {
    version: u32,
    rules: Vec<RewriteRule>,
    patterns: Vec<Vec<u32>>,
}

// Save algorithm
let bytes = bincode::serialize(&algorithm)?;
std::fs::write("algorithm.bin", bytes)?;

// Load algorithm
let bytes = std::fs::read("algorithm.bin")?;
let algorithm: SerializedAlgorithm = bincode::deserialize(&bytes)?;
```

**Recommendation:** Use **`bincode`** for fast, compact serialization of evolved algorithms.

---

## 9. Profiling and Optimization

### 9.1 Performance Measurement

**Key Crates:**
- **`criterion`** (v0.5+): Statistical benchmarking
- **`dhat`** (v0.3+): Heap profiling
- **`pprof`** (v0.13+): CPU profiling

```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion};

fn benchmark_annealing(c: &mut Criterion) {
    c.bench_function("anneal_text", |b| {
        b.iter(|| {
            let result = anneal(black_box(&input_text));
            black_box(result)
        })
    });
}

criterion_group!(benches, benchmark_annealing);
criterion_main!(benches);
```

### 9.2 Compile-Time Optimization

**Cargo.toml settings:**

```toml
[profile.release]
opt-level = 3
lto = "fat"              # Link-time optimization
codegen-units = 1        # Better optimization, slower compile
panic = "abort"          # Smaller binary
strip = true             # Remove debug symbols

[profile.release.package."*"]
opt-level = 3

# For GPU targets
[profile.gpu]
inherits = "release"
opt-level = 3
```

---

## 10. Testing Strategy

### 10.1 Property-Based Testing

**Key Crates:**
- **`proptest`** (v1.4+): Property-based testing
- **`quickcheck`** (v1.0+): QuickCheck port

```rust
use proptest::prelude::*;

proptest! {
    #[test]
    fn annealing_preserves_token_count(text in "\\PC{1,100}") {
        let tokens = tokenize(&text);
        let result = anneal(&tokens);
        // Property: output should have <= input tokens
        assert!(result.len() <= tokens.len());
    }
}
```

### 10.2 Fuzzing

**Key Crates:**
- **`cargo-fuzz`** (v0.11+): Fuzzing framework
- **`afl`** (v0.13+): American Fuzzy Lop integration

---

## 11. Recommended Project Structure

```
rust-server/src/
├── semantic_annealing/
│   ├── mod.rs              # Public API
│   ├── tokenizer.rs        # SentencePiece wrapper
│   ├── program_stream.rs   # ProgramStream and ProgramItem
│   ├── rules.rs            # RewriteRule definitions
│   ├── pattern_matcher.rs  # Aho-Corasick integration
│   ├── annealing.rs        # Core annealing loop
│   ├── forth_vm.rs         # Stack-based VM
│   ├── state.rs            # State management and backtracking
│   └── gpu/
│       ├── mod.rs
│       ├── cuda.rs         # CUDA implementation
│       └── wgpu.rs         # WebGPU fallback
├── evolution/
│   ├── mod.rs
│   ├── algorithm.rs        # Algorithm representation
│   ├── fitness.rs          # Fitness evaluation
│   ├── genetic_ops.rs      # Crossover, mutation
│   └── corpus.rs           # Corpus management
└── benchmarks/
    └── annealing_bench.rs
```

---

## 12. Implementation Roadmap

### Phase 1: Core Runtime (2-3 weeks)
1. **Week 1:** 
   - Set up `ProgramStream` with fixed buffers
   - Integrate `tokenizers` for SentencePiece
   - Implement basic Forth VM

2. **Week 2:**
   - Implement pattern matching with `aho-corasick`
   - Build rewrite rule system
   - Create annealing loop with backtracking

3. **Week 3:**
   - Optimize memory layout (SoA)
   - Add SIMD to VM
   - Benchmark and profile

### Phase 2: GPU Port (1-2 weeks)
4. **Week 4:**
   - Port annealing loop to CUDA with `cudarc`
   - Implement batch processing

5. **Week 5:**
   - Optimize GPU kernel
   - Add `wgpu` fallback

### Phase 3: Evolutionary Engine (2-3 weeks)
6. **Week 6:**
   - Set up `genevo` framework
   - Implement fitness evaluation with corpus

7. **Week 7:**
   - Add parallel evaluation with `rayon`
   - Implement genetic operators

8. **Week 8:**
   - Integrate with teacher LLM
   - Run initial evolution experiments

### Phase 4: Optimization (ongoing)
- Profile and optimize hot paths
- Tune genetic algorithm parameters
- Expand corpus and validation

---

## 13. Key Dependencies Summary

**Essential Crates:**

```toml
[dependencies]
# Tokenization
tokenizers = "0.15"

# Pattern matching (already present)
aho-corasick = "1.1"

# SIMD
wide = "0.7"

# Genetic algorithms
genevo = "0.7"
rayon = "1.8"

# Serialization (already present)
serde = { version = "1.0", features = ["derive"] }
bincode = "1.3"

# GPU (optional, choose one)
cudarc = "0.11"  # NVIDIA
# OR
wgpu = "0.19"    # Cross-platform

# Zero-allocation helpers
arrayvec = "0.7"
smallvec = "1.13"

# Utilities
priority-queue = "2.0"
memmap2 = "0.9"
rkyv = "0.7"

[dev-dependencies]
criterion = "0.5"
proptest = "1.4"
```

---

## 14. Critical Design Decisions

### 14.1 Memory Strategy
**Decision:** Use fixed-size arrays for `ProgramStream` with compile-time `MAX_ITEMS` constant.

**Rationale:** 
- Guarantees zero allocation in hot path
- Enables GPU porting
- Predictable performance

**Trade-off:** Limited to fixed maximum program size. Mitigation: Set `MAX_ITEMS = 4096` (sufficient for most texts).

### 14.2 GPU Strategy
**Decision:** Start with CPU implementation, design for GPU portability.

**Rationale:**
- Easier debugging on CPU
- GPU port is straightforward if memory layout is correct
- Can validate correctness before GPU optimization

**Trade-off:** Delayed GPU performance. Mitigation: Use SIMD on CPU for intermediate speedup.

### 14.3 Pattern Matching Strategy
**Decision:** Use Aho-Corasick for multi-pattern matching of token sequences.

**Rationale:**
- Already in dependencies
- O(n + m + z) complexity (n = text length, m = pattern length, z = matches)
- Well-tested and fast

**Trade-off:** Not optimized for very short patterns. Mitigation: Combine with direct comparison for patterns < 3 tokens.

### 14.4 Backtracking Strategy
**Decision:** Ring buffer of last N states with full state copies.

**Rationale:**
- Simple implementation
- Fast if state is small
- Predictable memory usage

**Trade-off:** Memory usage scales with state size. Mitigation: Keep `ProgramStream` compact (< 4KB).

---

## 15. Rust-Specific Advantages

### 15.1 Zero-Cost Abstractions
Rust's ownership system and zero-cost abstractions allow high-level code that compiles to optimal machine code:

```rust
// High-level iterator code
let score: f32 = bids
    .iter()
    .filter(|b| !inhibited.contains(&b.rule_id))
    .map(|b| b.score)
    .sum();

// Compiles to tight loop with no overhead
```

### 15.2 Fearless Concurrency
The evolutionary engine can safely parallelize fitness evaluation:

```rust
// Automatically safe parallel iteration
let fitness: Vec<f32> = population
    .par_iter()
    .map(|algo| evaluate(algo))
    .collect();
```

### 15.3 GPU Interop
Rust's C FFI and memory layout control make GPU integration straightforward:

```rust
#[repr(C)]
struct ProgramItem {
    instruction: u8,
    token_id: u32,
    score: f32,
}

// Can be directly copied to GPU memory
```

### 15.4 Compile-Time Guarantees
Rust's type system can encode invariants:

```rust
// Ensure program stream never exceeds capacity
struct ProgramStream<const N: usize> {
    items: [ProgramItem; N],
    len: usize,
}

impl<const N: usize> ProgramStream<N> {
    fn push(&mut self, item: ProgramItem) -> Result<(), CapacityError> {
        if self.len >= N {
            return Err(CapacityError);
        }
        self.items[self.len] = item;
        self.len += 1;
        Ok(())
    }
}
```

---

## 16. Potential Challenges and Mitigations

### Challenge 1: GPU Debugging
**Issue:** GPU code is hard to debug.

**Mitigation:**
- Develop and validate on CPU first
- Use GPU printf debugging (CUDA)
- Implement CPU reference implementation for comparison

### Challenge 2: Rule Explosion
**Issue:** Genetic algorithm may generate too many rules.

**Mitigation:**
- Add rule count to fitness function (parsimony pressure)
- Implement rule pruning based on usage statistics
- Use hierarchical rule organization

### Challenge 3: Local Optima in Evolution
**Issue:** GA may get stuck in local optima.

**Mitigation:**
- Use island model with multiple populations
- Implement simulated annealing in GA
- Periodic injection of random individuals

### Challenge 4: Corpus Size
**Issue:** Large corpus may not fit in memory.

**Mitigation:**
- Use memory-mapped files with `memmap2`
- Implement streaming evaluation
- Use mini-batch fitness evaluation

---

## 17. Next Steps

1. **Prototype Core Loop:** Implement basic annealing loop with dummy rules to validate architecture
2. **Benchmark Memory Layout:** Test different `ProgramStream` representations for cache performance
3. **Integrate Tokenizer:** Add `tokenizers` crate and test with real text
4. **Build Pattern Matcher:** Implement rule matching with `aho-corasick`
5. **Create Test Corpus:** Prepare small corpus with teacher embeddings for validation

---

## 18. References and Resources

### Rust Performance
- [The Rust Performance Book](https://nnethercote.github.io/perf-book/)
- [Rust SIMD Guide](https://rust-lang.github.io/packed_simd/perf-guide/)

### GPU Programming
- [cudarc Documentation](https://docs.rs/cudarc/)
- [wgpu Tutorial](https://sotrh.github.io/learn-wgpu/)

### Genetic Algorithms
- [genevo Examples](https://github.com/innoave/genevo/tree/master/examples)
- [Genetic Algorithms in Rust](https://depth-first.com/articles/2019/01/22/rust-and-webassembly-from-scratch-hello-world-with-strings/)

### Pattern Matching
- [Aho-Corasick Algorithm](https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm)
- [aho-corasick crate docs](https://docs.rs/aho-corasick/)

---

## Conclusion

Rust provides excellent tools and patterns for implementing the Semantic Annealing Engine:

- **Zero-allocation runtime** is natural with Rust's ownership system
- **GPU portability** is achievable with `cudarc` or `wgpu`
- **Pattern matching** is efficient with `aho-corasick`
- **Evolutionary algorithms** are well-supported by `genevo` and `rayon`
- **Performance** can match or exceed C++ with proper optimization

The main advantages over other languages:
- **Memory safety** without garbage collection overhead
- **Fearless concurrency** for parallel fitness evaluation
- **Zero-cost abstractions** for maintainable high-performance code
- **Strong ecosystem** for scientific computing and GPU programming

**Estimated development time:** 8-10 weeks for full implementation with optimization.

**Recommended starting point:** Build CPU-based prototype with `tokenizers`, `aho-corasick`, and fixed-size buffers, then optimize and port to GPU.