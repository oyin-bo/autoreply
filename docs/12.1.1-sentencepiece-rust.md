# SentencePiece Rust Implementation: High-Level Plan

**Note on sources:**

The C++ sources are available checked in to this repository at [sentencepiece/](../sentencepiece/).

The Gemini 3 data is available in [gemini-data/](../gemini-data/).

> **Objective:** Build a production-ready Rust implementation of SentencePiece inference for tokenization in the Autoreply MCP server. This serves as the reference implementation for subsequent Go and JavaScript ports.

**Document Status:** High-level implementation plan based on analysis of SentencePiece C++ source code and requirements from [12-sentencepiece-fuzzy-search-analysis.md](../12-sentencepiece-fuzzy-search-analysis.md) and [12.1-sentencepiece-quick-start.md](../12.1-sentencepiece-quick-start.md).

---

## Executive Summary

### Goals
1. **Functional parity** with official SentencePiece C++ implementation (inference only)
2. **Performance target:** <1ms p95 tokenization for 280-character inputs
3. **Memory efficiency:** Zero heap allocations per request after warm-up
4. **Platform support:** Native Rust (stable 1.78+)
5. **Integration ready:** Drop-in module for `rust-server/` semantic search pipeline

### Success Criteria
- Bit-exact token output matching C++ reference on multilingual test suite
- <50KB memory footprint per concurrent request
- Clean API surface for encoding text to token IDs

---

## 1. Core Components

### 1.1 Key Modules

The implementation requires the following functional areas:

- **Model loader**: Parse protobuf `.model` files and extract vocabulary, scores, normalization rules, and special token metadata
- **Normalizer**: Apply Unicode normalization (NFKC) and character-mapping rules from the model
- **Trie structure**: Fast prefix matching for vocabulary lookups during tokenization
- **Tokenizer**: Viterbi algorithm for unigram segmentation; optional BPE support
- **Error handling**: Idiomatic error types for model loading and encoding failures

### 1.2 Public API Surface

The crate should expose:
- An encoder function that accepts text and returns token IDs
- Model loading from file paths or byte buffers
- Configuration for BOS/EOS insertion and other model-specific options

### 1.3 Suggested Dependencies

**Core libraries:**
- `prost` + `prost-build` for protobuf parsing
- `unicode-normalization` for NFKC
- `smallvec` or similar for efficient short-lived allocations
- Standard error handling (`thiserror`, `anyhow`)

**Testing & benchmarking:**
- `criterion` for performance tracking
- `proptest` for property-based fuzzing

**Optional capabilities:**
- SIMD feature flag once core implementation is stable

---

## 2. Implementation Ordering

The work follows a natural dependency order, building from foundations upward.

### Step 1: Project Setup

**Goal:** Establish crate structure and protobuf codegen

**Activities:**
- Initialize Rust crate under `/rust-server/sentencepiece/`
- Configure `prost-build` in build script to generate types from SentencePiece proto definitions
- Set up test fixtures directory with sample `.model` files and expected outputs
- Establish CI pipeline for tests, linting, and formatting

**Exit criteria:** Build succeeds and protobuf types are accessible

### Step 2: Model Loading

**Goal:** Parse `.model` files and extract vocabulary with metadata

**Activities:**
- Implement protobuf parsing to extract vocabulary pieces and their scores
- Identify special token IDs (UNK, BOS, EOS, PAD) from trainer spec, matching logic found in `src/sentencepiece_processor.cc`
- Extract normalization rules from normalizer spec, mirroring rule handling in `src/normalizer.cc`
- Validate model integrity (non-empty vocab, required fields present)
- Add unit tests comparing parsed data against known fixtures

**Exit criteria:** Can load a `.model` file and enumerate vocabulary pieces with correct scores

### Step 3: Text Normalization

**Goal:** Apply Unicode and character-mapping normalization

**Activities:**
- Integrate Unicode NFKC normalization
- Build rule matcher from model's character-mapping table following `src/normalizer.cc`
- Handle whitespace escaping and dummy prefix insertion
- Maintain position map from normalized text back to original indices
- Test against C++ reference outputs for various Unicode inputs sourced from upstream `normalizer_test.cc`

**Exit criteria:** Normalization output matches C++ SentencePiece on multilingual test cases

#### Planned zero-allocation refactor
- **Remove interim strings:** Update `rust-server/src/sentencepiece/normalizer.rs` so normalization rules store character slices instead of cloned `String` values.
- **Reuse workspace only:** Drop the cached `string` field from `NormalizerWorkspace` and expose normalization results via `chars()`/`positions()`; provide an on-demand helper to build owned text for tests.
- **Adjust tests:** Rewrite local tests in `rust-server/src/sentencepiece/normalizer.rs` to compare collected characters, keeping coverage identical while avoiding mandatory heap use.
- **Tokenizer unchanged:** `rust-server/src/sentencepiece/tokenizer.rs` continues reading `normalized.chars()` sequentially, so no API changes are needed downstream.
- **Justification:** Eliminates per-call heap traffic, keeping the zero-allocation promise while preserving existing behavior through updated slice-based assertions.

### Step 4: Vocabulary Trie

**Goal:** Enable fast prefix matching for tokenization

**Activities:**
- Implement or integrate a trie structure for vocabulary lookups, aligning with the double-array structure in `src/darts_clone.cc`
- Support common-prefix search over UTF-8 byte sequences with iterators modeled after the C++ `DA::Trie::CommonPrefixSearch` implementation
- Optimize for lookup performance (consider double-array or similar compact representation drawing from build strategy in [12.1.1-sentencepiece-rust.md](./12.1.1-sentencepiece-rust.md))
- Test lookup correctness against vocabulary fixtures and compare node counts with the C++ `encoder/model_interface.cc`

#### Data structures to consider:
- **Static (model lifetime):**
  - **Vocabulary catalog:** Might maintain a consolidated view of each piece’s text, identifier, score, and classification so trie construction and scoring share the same source of information.
  - **Vocabulary shared byte storage:** For all piece text inside a single backing buffer that the catalog references, avoiding duplicated strings across the model.
  - **Prefix lookup table:** Compact trie-like index derived from the catalog to answer common-prefix searches efficiently.
  - **Metadata sidecar:** Collects constants like the unknown-piece identifier and penalty values needed when interpreting trie matches.
- **Per-request scratch:**
  - **Lookup scratchpad:** Reusable workspace sized to the largest expected match set so prefix searches can reuse memory between requests.
  - **Candidate staging area:** Temporary match details—such as start position, character length, identifier, and adjusted score—before Step 5 consumes them.

**Exit criteria:** Trie returns correct token IDs for prefix queries

### Step 5: Unigram Tokenizer

**Goal:** Implement Viterbi-based segmentation

**Activities:**
- Build forward dynamic-programming pass over normalized text, mirroring `sentencepiece::unigram::Lattice::Forward` in `src/unigram_model.cc`
- Score candidate segmentations using vocabulary log probabilities and penalties from `src/unigram_model.cc`
- Backtrack to extract optimal token sequence, ensuring tie-breaking matches `SentencePieceProcessor::PopulateResult`
- Handle unknown tokens and edge cases (empty input, pure whitespace) using heuristics described in [12.1.6-sentencepiece-rusto3.md](./12.1.6-sentencepiece-rusto3.md)
- Cross-check outputs against C++ golden data and the upstream `self_test_data` corpus

#### Data structures to consider:
- **Static (model lifetime):**
  - **Scoring constants:** `min_score`, `max_score`, `unk_id`, and penalties derived during Step 2, reused when `PopulateNodes()` scores matches in `unigram_model.cc`.
  - **Trie-derived match budget:** `trie_results_size_` from Step 4 bounds candidate fan-out and informs preallocation for dynamic-programming buffers.
  - **Control flags:** Encoder version switches (optimized vs. lattice) and sampling configuration preserved on the model instance.
- **Per-request scratch:**
  - **`Lattice` arena:** Holds `begin_nodes_`, `end_nodes_`, UTF-8 surfaces, and `model::FreeList<Node>` when using the classic Viterbi path (`Lattice::Viterbi()`, `ForwardAlgorithm()` in `unigram_model.cc`).
  - **Dynamic-programming tables:** Temporary `alpha`, `beta`, entropy accumulators, and hypothesis queues allocated per request for sampling or n-best paths.
  - **Optimized backpointers:** For the `EncodeOptimized()` path, a `best_path_ends_at` array over UTF-8 indices replaces the lattice while reusing the same score inputs.
- **Per-request outputs:**
  - **Candidate sequence:** `EncodeResult` (pairs of piece span and id) produced after backtracking; later converted into the public token list returned to callers.
  - **Auxiliary scores:** Optional inclusion probabilities or entropy values when sampling APIs are invoked, derived from the same per-request buffers.

> **Note:** Public APIs can return slices that borrow these buffers for read-only use. Callers needing longer-lived or mutable copies should clone before invoking the tokenizer again, since subsequent calls reuse the same storage.

**Exit criteria:** Token sequences match C++ reference exactly on test suite

### Step 6: Integration & Testing

**Goal:** Wire components into cohesive API and validate end-to-end

**Activities:**
- Expose public encoding function with ergonomic interface
- Add property-based tests (fuzzing) to ensure no panics on arbitrary inputs
- Set up performance benchmarks tracking latency and allocations
- Validate memory footprint stays within target (<50KB per request)
- Document public API and usage examples

**Exit criteria:** All integration tests pass, benchmarks meet performance targets